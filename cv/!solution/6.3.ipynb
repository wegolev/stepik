{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Семинар: cлой нормализации\n",
    "#### 2 из 11 шагов\n",
    "\n",
    "В этом уроке мы детально изучим слои нормализации.\n",
    "\n",
    "Самая популярная версия слоя нормализации - слой нормализации \"по батчу\" (batch-norm слой).\n",
    "\n",
    "В данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета принимается равным 0.\n",
    "- Параметр Гамма принимается равным 1.\n",
    "- Функция должна корректно работать только на этапе обучения.\n",
    "- Вход имеет размерность число элементов в батче * длина каждого инстанса.\n",
    " \n",
    "Очень внимательно посмотрите на определение функции, вычисляющей std.\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, eps):\n",
    "    mean = input_tensor.mean(dim=0)\n",
    "    var = input_tensor.var(dim=0, unbiased=False)\n",
    "    \n",
    "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
    "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "# import numpy as np\n",
    "# all_correct = True\n",
    "# for eps_power in range(10):\n",
    "#     eps = np.power(10., -eps_power)\n",
    "#     batch_norm.eps = eps\n",
    "#     batch_norm_out = batch_norm(input_tensor)\n",
    "#     custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
    "\n",
    "#     all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
    "#     all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
    "# print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 из 11 шагов\n",
    "\n",
    "Немного обобщим функцию с предыдущего шага - добавим возможность задавать параметры Бета и Гамма.\n",
    "\n",
    "На данном шаге вам требуется реализовать функцию батч-нормализации без использования стандартной функции со следующими упрощениями:\n",
    "\n",
    "Функция должна корректно работать только на этапе обучения.\n",
    "Вход имеет размерность число элементов в батче * длина каждого инстанса.\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 7\n",
    "batch_size = 5\n",
    "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
    "    # Шаг 1: Вычисление среднего значения\n",
    "    mean = input_tensor.mean(dim=0)\n",
    "    \n",
    "    # Шаг 2: Вычетание среднего значения\n",
    "    centered_input = input_tensor - mean\n",
    "    \n",
    "    # Шаг 3: Вычисление дисперсии и нормализация\n",
    "    variance = centered_input.pow(2).mean(dim=0)\n",
    "    stddev = torch.sqrt(variance + eps)\n",
    "    normalized_input = centered_input / stddev\n",
    "    \n",
    "    # Шаги 4 и 5: Применение весового коэффициента и смещения\n",
    "    normed_tensor = weight * normalized_input + bias\n",
    "    \n",
    "    return normed_tensor\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "# batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "# batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "# batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "# batch_norm_out = batch_norm(input_tensor)\n",
    "# custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
    "# print(torch.allclose(batch_norm_out, custom_batch_norm_out, 1e-3) \\\n",
    "#       and batch_norm_out.shape == custom_batch_norm_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 из 11 шагов\n",
    "\n",
    "Избавимся еще от одного упрощения - реализуем работу слоя батч-нормализации на этапе предсказания.\n",
    "\n",
    "На этом этапе вместо статистик по батчу будем использовать экспоненциально сглаженные статистики из истории обучения слоя.\n",
    "\n",
    "В данном шаге вам требуется реализовать полноценный класс батч-нормализации без использования стандартной функции, принимающий на вход двумерный тензор. Осторожно, расчёт дисперсии ведётся по смещенной выборке, а расчет скользящего среднего по несмещенной.\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Настройки\n",
    "input_size = 3\n",
    "batch_size = 5\n",
    "eps = 1e-1\n",
    "\n",
    "class CustomBatchNorm1d:\n",
    "    def __init__(self, weight, bias, eps, momentum):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = torch.zeros(weight.shape[0])\n",
    "        self.running_var = torch.ones(weight.shape[0])\n",
    "        self.is_training = True\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        if self.is_training:\n",
    "            # Расчёт текущих средних и дисперсий (несмещенная)\n",
    "            batch_mean = input_tensor.mean(dim=0)\n",
    "            batch_var = ((input_tensor - batch_mean) ** 2).mean(dim=0)\n",
    "            # batch_var = torch.var(input_tensor, dim=0, unbiased=False)\n",
    "            \n",
    "            # Обновление экспоненциальных средних (смещенная оценка дисперсии)\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var * (batch_size / (batch_size - 1))\n",
    "\n",
    "            # Нормализация\n",
    "            normed_tensor = (input_tensor - batch_mean) / torch.sqrt(batch_var + self.eps) * self.weight + self.bias\n",
    "        else:\n",
    "            # Расчет и накопление экспоненциально сглаженных средних и дисперсий\n",
    "            normed_tensor = (input_tensor - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "            # Применение весов и смещений\n",
    "            normed_tensor = self.weight * normed_tensor + self.bias\n",
    "\n",
    "        return normed_tensor\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_training = False\n",
    "\n",
    "\n",
    "# Стандартный слой батч-нормализации\n",
    "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
    "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
    "batch_norm.momentum = 0.5\n",
    "\n",
    "# Пользовательский слой батч-нормализации\n",
    "custom_batch_norm1d = CustomBatchNorm1d(\n",
    "    batch_norm.weight.data,\n",
    "    batch_norm.bias.data,\n",
    "    eps,\n",
    "    batch_norm.momentum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "custom_output\n",
      " tensor([[ 0.8354,  0.6386, -1.1657],\n",
      "        [ 0.3218,  0.4137,  2.2445],\n",
      "        [-1.0693,  0.7540, -0.8937],\n",
      "        [-0.4910,  0.5366,  1.5385],\n",
      "        [ 0.7688,  0.2119, -2.7308]])\n",
      "norm_output\n",
      " tensor([[ 0.8354,  0.6386, -1.1657],\n",
      "        [ 0.3218,  0.4137,  2.2445],\n",
      "        [-1.0693,  0.7540, -0.8937],\n",
      "        [-0.4910,  0.5366,  1.5385],\n",
      "        [ 0.7688,  0.2119, -2.7308]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "all_correct = True\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "\n",
    "batch_norm.eval()\n",
    "custom_batch_norm1d.eval()\n",
    "\n",
    "for i in range(8):\n",
    "    torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
    "    norm_output = batch_norm(torch_input)\n",
    "    custom_output = custom_batch_norm1d(torch_input)\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
    "        and norm_output.shape == custom_output.shape\n",
    "\n",
    "print(all_correct)\n",
    "print(\"custom_output\\n\", custom_output)\n",
    "print(\"norm_output\\n\", norm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 из 11 шагов\n",
    "\n",
    "Как вы могли убедиться, реализовать батч-норм слой на этапе предсказания не так просто, поэтому в дальнейших шагах этого урока мы больше не будем требовать реализовать эту часть.\n",
    " \n",
    "Слой батч-нормализации существует для входа любой размерности.\n",
    "\n",
    "В данном шаге рассмотрим его для входа из многоканальных двумерных тензоров, например, изображений.\n",
    "\n",
    "Если вытянуть каждый канал картинки в вектор, то вход будет трехмерным:\n",
    "\n",
    "- количество картинок в батче\n",
    "- число каналов в каждой картинке\n",
    "- число пикселей в картинке\n",
    "\n",
    "picture​\n",
    "\n",
    "Процесс нормализации:\n",
    "\n",
    "- Вход разбивается на срезы, параллельные синей части. То есть, каждый срез - это все пиксели всех изображений по одному из каналов.\n",
    "- Для каждого среза считаются мат. ожидание и дисперсия.\n",
    "- Каждый срез нормализуется независимо.\n",
    " \n",
    "\n",
    "На данном шаге вам предлагается реализовать батч-норм слой для четырехмерного входа (например, батч из многоканальных двумерных картинок) без использования стандартной реализации со следующими упрощениями:\n",
    "\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- Функция должна корректно работать только на этапе обучения.\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "input_channels = 3\n",
    "batch_size = 3\n",
    "height = 10\n",
    "width = 10\n",
    "\n",
    "batch_norm_2d = nn.BatchNorm2d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, height, width, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_batch_norm2d(input_tensor, eps):\n",
    "    #mean = input_tensor.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    mean = torch.tensor([torch.mean(input_tensor[:,i,:,:]) for i in range(input_tensor.shape[1])])\n",
    "    mean = mean.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    # var = input_tensor.var(dim=(0, 2, 3), unbiased=False, keepdim=True)\n",
    "    var = torch.tensor([torch.var(input_tensor[:,i,:,:], unbiased=False) for i in range(input_tensor.shape[1])])\n",
    "    var = var.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    normalized_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "norm_output = batch_norm_2d(input_tensor)\n",
    "custom_output = custom_batch_norm2d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объяснение шагов:\n",
    "- Вычисление среднего значения (mean): Мы используем метод .mean() для вычисления среднего значения по всем изображениям в батче, а также по высоте и ширине каждого изображения. Параметры dim=(0, 2, 3) указывают, что средние будут рассчитаны по первым трем измерениям (то есть по батчу, высоте и ширине соответственно).keepdim=True сохраняет размеры тензора, чтобы он мог быть правильно использован при вычитании.\n",
    "- Вычисление дисперсии (var): Аналогично среднему значению, мы вычисляем дисперсию для каждого канала, используя метод .var(). Здесь важно использовать параметр unbiased=False, так как стандартная реализация батч-нормализации использует несмещенную оценку дисперсии.\n",
    "- Нормализация: После того как мы получили среднее и дисперсию, выполняем нормализацию каждого элемента тензора, вычитая среднее и деля на корень из дисперсии плюс небольшое число eps для числовой стабильности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за того, что платформа не поддерживает листы в dim=, пришлось искать обходные пути. Итого, нашёл три способа.\n",
    "\n",
    "**Способ 1**, не поддерживвается платформой.\n",
    "`mean = input_tensor.mean(dim=(0, 2, 3), keepdim=True)`\n",
    "Аналогично для var (не забываем unbiased=False), и для normed_tensor - совсем просто.\n",
    "Попробуйте этот способ, полагаю, знать его надо.\n",
    "\n",
    "**Способ 2**, с конкатенацией через  .cat и .unsqueeze. Имхо коряво, но для понимания самое то.\n",
    "Оъясню опять-таки только для mean:\n",
    "\n",
    "Цикл по каналам:\n",
    "`for i in range(input_tensor.shape[1]):` Цикл проходит по каждому каналу входного тензора (input_tensor).\n",
    "\n",
    "Внутри - вычисление среднего по каналу:\n",
    "`torch.mean(input_tensor[:,i,:,:])`. Вычисляет среднее значение всех элементов в i-м канале (по всем элементам batch, \"высоте\" и \"ширине\"). Результат — скалярное значение (одно число), представляющее среднее значение по каналу.\n",
    "\n",
    "Добавление размерности:\n",
    "unsqueeze(0): Добавляет новую ось (размерность) на нулевую позицию, делая скалярное значение тензором с размерностью (1,).\n",
    "\n",
    "Объединение тензоров:\n",
    "`torch.cat([...], dim=0)`: Объединяет полученные тензоры среднего значения для всех каналов вдоль первой оси (ось \"batch\"). Результат — тензор mean с размерностью (input_channels,).\n",
    "\n",
    "Для var всё аналогично, не забываем unbiased=False.\n",
    "\n",
    "Для normed_tensor - unsqueeze(1) и dim=1 (это подсказка не моя, а команды курса).\n",
    " \n",
    "\n",
    "**Способ 3**, без склейки, простым преобразованием листа в тензор.\n",
    "\n",
    "`mean = torch.tensor([torch.mean(input_tensor[:,i,:,:]) for i in range(input_tensor.shape[1])])`\n",
    "\n",
    "Для var всё аналогично, не забываем unbiased=False.\n",
    "\n",
    "Для normed_tensor применяем .stack - `torch.stack([...], dim=1)`. Создаём новый тензор, \"складывая\" тензоры из списка вдоль второй оси (dim=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7 из 11 шагов\n",
    "\n",
    "Идея, лежащая в основе слоя нормализации \"по каналу\", что сеть должна быть независимой от контраста исходного изображения.\n",
    "\n",
    "Нормализация \"по каналу\" работает независимо по каждому изображению батча.\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по каналу\" без использования стандартного слоя со следующими упрощениями:\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- Требуется реализация только этапа обучения.\n",
    "- Нормализация делается по всем размерностям входа, кроме нулевой.\n",
    "\n",
    "**Обратите внимание, что размерность входа на данном шаге не фиксирована.**\n",
    "\n",
    "Уточним, что в слое нормализации \"по каналу\" статистики считаются по всем размерностям, кроме нулевой.\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "\n",
    "def custom_layer_norm(input_tensor, eps):\n",
    "    dims = list(range(1, len(input_tensor.size())))\n",
    "    mean = input_tensor.mean(dims, keepdim=True)\n",
    "    var = input_tensor.var(dims, unbiased=False, keepdim=True)\n",
    "    \n",
    "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    return normed_tensor\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "all_correct = True\n",
    "for dim_count in range(3, 9):\n",
    "    input_tensor = torch.randn(*list(range(3, dim_count + 2)), dtype=torch.float)\n",
    "    layer_norm = nn.LayerNorm(input_tensor.size()[1:], elementwise_affine=False, eps=eps)\n",
    "\n",
    "    norm_output = layer_norm(input_tensor)\n",
    "    custom_output = custom_layer_norm(input_tensor, eps)\n",
    "\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-2)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Определение осей для расчета статистики:\n",
    "```dims = list(range(1, len(input_tensor.size())))```\n",
    "\n",
    "Мы создаем список индексов всех размерностей, начиная с 1-й оси (пропуская первую ось, соответствующую батчу).\n",
    "\n",
    "2. асчет среднего значения:```mean = input_tensor.mean(dims, keepdim=True)```\n",
    "\n",
    "Среднее значение рассчитывается по всем указанным осям, но благодаря параметру keepdim=True сохраняются те же самые размеры, что и у исходного тензора.\n",
    "\n",
    "3. Расчет дисперсии:```var = input_tensor.var(dims, unbiased=False, keepdim=True)```\n",
    "\n",
    "Дисперсия также считается по тем же осям, причем используется несмещенная оценка дисперсии (unbiased=False), что соответствует поведению стандартной реализации.\n",
    "\n",
    "4. Нормализация:```normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)```\n",
    "\n",
    "Нормализация выполняется путем вычитания среднего значения и деления на квадратный корень из дисперсии плюс малую константу eps для численной устойчивости.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "\n",
    "def custom_layer_norm(input_tensor, eps):\n",
    "    # Определяем количество каналов (первая размерность после батча)\n",
    "    num_dims = len(input_tensor.size())\n",
    "    \n",
    "    # Создадим списки для хранения средних значений и дисперсий\n",
    "    means = []\n",
    "    vars = []\n",
    "    \n",
    "    # Проходим по каждому элементу в батче\n",
    "    for i in range(input_tensor.size(0)):\n",
    "        # Берём срез по текущей позиции в батче\n",
    "        slice_mean = torch.mean(input_tensor[i])\n",
    "        means.append(slice_mean)\n",
    "        \n",
    "        # Рассчитываем дисперсию для текущего элемента батча\n",
    "        slice_var = torch.var(input_tensor[i], unbiased=False)\n",
    "        vars.append(slice_var)\n",
    "    \n",
    "    # Преобразуем списки в тензоры\n",
    "    means_tensor = torch.stack(means)\n",
    "    vars_tensor = torch.stack(vars)\n",
    "    \n",
    "    # Приводим размеры тензоров к нужному виду\n",
    "    means_tensor = means_tensor.view(-1, *([1] * (num_dims - 1)))\n",
    "    vars_tensor = vars_tensor.view(-1, *([1] * (num_dims - 1)))\n",
    "    \n",
    "    # Выполним нормализацию\n",
    "    normed_tensor = (input_tensor - means_tensor) / torch.sqrt(vars_tensor + eps)\n",
    "    \n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "all_correct = True\n",
    "for dim_count in range(3, 9):\n",
    "    input_tensor = torch.randn(*list(range(3, dim_count + 2)), dtype=torch.float)\n",
    "    layer_norm = nn.LayerNorm(input_tensor.size()[1:], elementwise_affine=False, eps=eps)\n",
    "\n",
    "    norm_output = layer_norm(input_tensor)\n",
    "    custom_output = custom_layer_norm(input_tensor, eps)\n",
    "\n",
    "    # print(norm_output)\n",
    "    # print(custom_output)\n",
    "\n",
    "    all_correct &= torch.allclose(norm_output, custom_output, 1e-2)\n",
    "    all_correct &= norm_output.shape == custom_output.shape\n",
    "print(all_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Не по каналу, а по батчу.** По каналу было прошлый раз. Сейчас проход по нулевой размерности input_tensor.\n",
    "\n",
    "@Илья_Шишов, да, проходим по батчу, а усредняем каналы. Возможно стоило написать \"по каналам\", раз уж прямой перевод \"канальная нормализация\" звучит не очень. Но с переводами всегда сложно.\n",
    "\n",
    "Все операции выполняются для размерности 0. `for j in range(0, input.size()[0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбор изменений:\n",
    "\n",
    "Цикл по батчу:\n",
    "```\n",
    "for i in range(input_tensor.size(0)):\n",
    "    slice_mean = torch.mean(input_tensor[i])\n",
    "    means.append(slice_mean)\n",
    "    \n",
    "    slice_var = torch.var(input_tensor[i], unbiased=False)\n",
    "    vars.append(slice_var)\n",
    "```\n",
    "Теперь цикл проходит по каждому элементу в батче (нулевое измерение), а не по каналам. Соответственно, для каждого элемента батча мы вычисляем среднее значение и дисперсию.\n",
    "\n",
    "Приведение форматов:\n",
    "```\n",
    "means_tensor = means_tensor.view(-1, *([1] * (num_dims - 1)))\n",
    "vars_tensor = vars_tensor.view(-1, *([1] * (num_dims - 1)))\n",
    "```\n",
    "Формируем тензоры таким образом, чтобы они имели нужное количество измерений. Первое измерение соответствует размеру батча, остальные — единичным размерам для совместимости с другими измерениями исходного тензора.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 из 11 шагов \n",
    "\n",
    "Нормализация \"по инстансу\" была изначально разработана для задачи style transfer. Идея, лежащая в основе этого слоя, что сеть должна быть независимой от контраста отдельных каналов исходного изображения.\n",
    "\n",
    "На этом шаге вам предлагается реализовать нормализацию \"по инстансу\" без использования стандартного слоя со следующими упрощениями:\n",
    "- Параметр Бета = 0.\n",
    "- Параметр Гамма = 1.\n",
    "- На вход подается трехмерный тензор (размер батча, число каналов, длина каждого канала инстанса).\n",
    "- Требуется реализация только этапа обучения.\n",
    "\n",
    "В слое нормализации \"по инстансу\" статистики считаются по последней размерности (по каждому входному каналу каждого входного примера).\n",
    "\n",
    "Sample Input: anything\n",
    "\n",
    "Sample Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "batch_size = 5\n",
    "input_channels = 2\n",
    "input_length = 30\n",
    "\n",
    "instance_norm = nn.InstanceNorm1d(input_channels, affine=False, eps=eps)\n",
    "\n",
    "input_tensor = torch.randn(batch_size, input_channels, input_length, dtype=torch.float)\n",
    "\n",
    "\n",
    "def custom_instance_norm1d(input_tensor, eps):\n",
    "    # Количество каналов\n",
    "    channels = input_tensor.size(1)\n",
    "    \n",
    "    # Список для хранения средних значений и дисперсий\n",
    "    means = []\n",
    "    vars = []\n",
    "    \n",
    "    # Проходим по каждому каналу\n",
    "    for c in range(channels):\n",
    "        # Берём срез по текущему каналу\n",
    "        channel_slice = input_tensor[:, c]\n",
    "        \n",
    "        # Рассчитываем среднее значение по длине канала\n",
    "        mean = torch.mean(channel_slice, dim=-1, keepdim=True)\n",
    "        means.append(mean)\n",
    "        \n",
    "        # Рассчитываем дисперсию по длине канала\n",
    "        var = torch.var(channel_slice, dim=-1, unbiased=False, keepdim=True)\n",
    "        vars.append(var)\n",
    "    \n",
    "    # Преобразуем списки в тензоры\n",
    "    means_tensor = torch.cat(means, dim=1)\n",
    "    vars_tensor = torch.cat(vars, dim=1)\n",
    "    \n",
    "    # Расширяем размеры тензоров до нужного вида\n",
    "    means_tensor = means_tensor.unsqueeze(-1)\n",
    "    vars_tensor = vars_tensor.unsqueeze(-1)\n",
    "    \n",
    "    # Выполним нормализацию\n",
    "    normed_tensor = (input_tensor - means_tensor) / torch.sqrt(vars_tensor + eps)\n",
    "    \n",
    "    return normed_tensor\n",
    "\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "norm_output = instance_norm(input_tensor)\n",
    "custom_output = custom_instance_norm1d(input_tensor, eps)\n",
    "print(torch.allclose(norm_output, custom_output, atol=1e-06) and norm_output.shape == custom_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот код реализует нормализацию \"по инстансу\" для трёхмерного тензора, где статистика считается по последнему измерению (длина каждого канала инстанса)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
